{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data\\preprocessed_train.csv')\n",
    "test = pd.read_csv('data\\preprocessed_test.csv')\n",
    "features_danish = pd.read_csv('manual_features\\danish_manual_features.csv', index_col=0)\n",
    "features_mipt = pd.read_csv('manual_features\\mipt_manual_features.csv', index_col=0)\n",
    "features_nn = pd.read_csv('manual_features/nn_manual_features.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_danish Index(['alpha', 'beta', 'count_entropy', 'crossing_points', 'entropy',\n",
      "       'flat_spots', 'frequency', 'intervals_mean', 'lumpiness', 'arch_lm',\n",
      "       'sparsity', 'stability', 'nperiods', 'seasonal_period', 'trend',\n",
      "       'spike', 'linearity', 'curvature'],\n",
      "      dtype='object'), lenght: 18\n",
      "features_mipt Index(['alpha', 'beta', 'count_entropy', 'crossing_points', 'entropy',\n",
      "       'flat_spots', 'frequency', 'hurst', 'intervals_mean', 'lumpiness',\n",
      "       'arch_lm', 'sparsity', 'stability', 'nperiods', 'seasonal_period',\n",
      "       'trend', 'spike', 'linearity', 'curvature'],\n",
      "      dtype='object'), lenght: 19\n",
      "features_nn Index(['alpha', 'beta', 'count_entropy', 'crossing_points', 'entropy',\n",
      "       'flat_spots', 'frequency', 'hurst', 'intervals_mean', 'intervals_sd',\n",
      "       'lumpiness', 'x_acf1', 'x_acf10', 'diff1_acf1', 'diff1_acf10',\n",
      "       'diff2_acf1', 'diff2_acf10', 'arch_lm', 'x_pacf5', 'diff1x_pacf5',\n",
      "       'diff2x_pacf5', 'sparsity', 'stability', 'nperiods', 'seasonal_period',\n",
      "       'trend', 'spike', 'linearity', 'curvature', 'e_acf1', 'e_acf10',\n",
      "       'unitroot_kpss', 'unitroot_pp'],\n",
      "      dtype='object') lenght: 33\n"
     ]
    }
   ],
   "source": [
    "print(f'features_danish {features_danish.columns}, lenght: {len(features_danish.columns)}')\n",
    "print(f'features_mipt {features_mipt.columns}, lenght: {len(features_mipt.columns)}')\n",
    "print(f'features_nn {features_nn.columns} lenght: {len(features_nn.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking difference in features\n",
    "unique_list3 = set(features_nn.columns) - (set(features_danish.columns) | set(features_mipt.columns))\n",
    "# Manually adding \"hurst\", because it wasn't added during logical operation\n",
    "unique_list3.update({'hurst'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column hurst dropped from features_mipt\n",
      "Column diff1_acf10 dropped from features_nn\n",
      "Column hurst dropped from features_nn\n",
      "Column diff2_acf10 dropped from features_nn\n",
      "Column e_acf1 dropped from features_nn\n",
      "Column intervals_sd dropped from features_nn\n",
      "Column x_acf1 dropped from features_nn\n",
      "Column diff1_acf1 dropped from features_nn\n",
      "Column diff2_acf1 dropped from features_nn\n",
      "Column x_acf10 dropped from features_nn\n",
      "Column diff2x_pacf5 dropped from features_nn\n",
      "Column x_pacf5 dropped from features_nn\n",
      "Column diff1x_pacf5 dropped from features_nn\n",
      "Column unitroot_pp dropped from features_nn\n",
      "Column unitroot_kpss dropped from features_nn\n",
      "Column e_acf10 dropped from features_nn\n"
     ]
    }
   ],
   "source": [
    "# Dropping different columns\n",
    "for df, name in zip([features_danish, features_mipt, features_nn], ['features_danish', 'features_mipt', 'features_nn']):\n",
    "    for col in unique_list3:\n",
    "        if col in df.columns:\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "            print(f'Column {col} dropped from {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_danish Index(['alpha', 'beta', 'count_entropy', 'crossing_points', 'entropy',\n",
      "       'flat_spots', 'frequency', 'intervals_mean', 'lumpiness', 'arch_lm',\n",
      "       'sparsity', 'stability', 'nperiods', 'seasonal_period', 'trend',\n",
      "       'spike', 'linearity', 'curvature'],\n",
      "      dtype='object'), lenght: 18\n",
      "features_mipt Index(['alpha', 'beta', 'count_entropy', 'crossing_points', 'entropy',\n",
      "       'flat_spots', 'frequency', 'intervals_mean', 'lumpiness', 'arch_lm',\n",
      "       'sparsity', 'stability', 'nperiods', 'seasonal_period', 'trend',\n",
      "       'spike', 'linearity', 'curvature'],\n",
      "      dtype='object'), lenght: 18\n",
      "features_nn Index(['alpha', 'beta', 'count_entropy', 'crossing_points', 'entropy',\n",
      "       'flat_spots', 'frequency', 'intervals_mean', 'lumpiness', 'arch_lm',\n",
      "       'sparsity', 'stability', 'nperiods', 'seasonal_period', 'trend',\n",
      "       'spike', 'linearity', 'curvature'],\n",
      "      dtype='object') lenght: 18\n"
     ]
    }
   ],
   "source": [
    "print(f'features_danish {features_danish.columns}, lenght: {len(features_danish.columns)}')\n",
    "print(f'features_mipt {features_mipt.columns}, lenght: {len(features_mipt.columns)}')\n",
    "print(f'features_nn {features_nn.columns} lenght: {len(features_nn.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in features 0\n"
     ]
    }
   ],
   "source": [
    "# Concating features\n",
    "features = pd.concat([features_danish, features_mipt, features_nn], axis=0)\n",
    "print(f'NaNs in features {np.sum(features.isna().sum())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting features\n",
    "features_sorted = features.sort_index(axis=0, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Unamed: 0 column\n",
    "train.drop(train.columns[0], inplace=True, axis=1)\n",
    "test.drop(test.columns[0], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting index and sorting\n",
    "train.set_index('naming_orig', inplace=True)\n",
    "train_sorted = train.sort_index(axis=0, ascending=True)\n",
    "test.set_index('naming_orig', inplace=True)\n",
    "test_sorted = test.sort_index(axis=0, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features to drop fetures list for train ['danish_atm_daily_111', 'danish_atm_daily_110', 'danish_atm_daily_112']\n",
      "Features to drop fetures list for train ['danish_atm_daily_111', 'danish_atm_daily_110', 'danish_atm_daily_112']\n"
     ]
    }
   ],
   "source": [
    "# Features to drop for train and test data\n",
    "drop_from_train = list(set(features.index)-set(train_sorted.index))\n",
    "print(f'Features to drop fetures list for train {drop_from_train}')\n",
    "drop_from_test = list(set(features.index)-set(test_sorted.index))\n",
    "print(f'Features to drop fetures list for train {drop_from_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping values from features\n",
    "features_train = features.drop(drop_from_train, axis=0)\n",
    "features_test = features.drop(drop_from_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatinating data\n",
    "train_conc = pd.concat([features_train, train_sorted], axis=1)\n",
    "test_conc = pd.concat([features_test, test_sorted], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping SMAPE columns\n",
    "train_conc.drop(['SMAPE_model', 'RMSE', 'SMAPE', 'RMSE_model'], axis=1, inplace=True)\n",
    "test_conc.drop(['SMAPE_model', 'RMSE', 'SMAPE', 'RMSE_model'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting index names\n",
    "train_conc.index.name = 'naming_orig'\n",
    "test_conc.index.name = 'naming_orig'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in train set 0\n",
      "NaNs in test set 0\n"
     ]
    }
   ],
   "source": [
    "# Cheking NaNs\n",
    "print(f'NaNs in train set {np.sum(train_conc.isna().sum())}')\n",
    "print(f'NaNs in test set {np.sum(test_conc.isna().sum())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving files\n",
    "test_conc.to_csv('data/ready_test.csv')\n",
    "train_conc.to_csv('data/ready_train.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
